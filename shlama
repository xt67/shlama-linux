#!/bin/bash

# [llama] shlama-linux - Your terminal llama
# Natural language -> safe Linux commands
# Powered by Ollama

set -e

# Configuration
CONFIG_FILE="$HOME/.config/shlama/config"
DEFAULT_MODEL="llama3.2"

# Load saved model from config, or use default
if [[ -f "$CONFIG_FILE" ]]; then
    SAVED_MODEL=$(cat "$CONFIG_FILE" 2>/dev/null)
fi
MODEL="${SHLAMA_MODEL:-${SAVED_MODEL:-$DEFAULT_MODEL}}"
OLLAMA_HOST="${OLLAMA_HOST:-http://localhost:11434}"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Print colored output
print_color() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

# Check if Ollama is running
check_ollama() {
    if curl -s --connect-timeout 2 "${OLLAMA_HOST}/api/tags" > /dev/null 2>&1; then
        return 0
    fi
    return 1
}

# Check if using remote Ollama
is_remote_ollama() {
    case "$OLLAMA_HOST" in
        http://localhost:*|http://127.0.0.1:*|"")
            return 1  # Local
            ;;
        *)
            return 0  # Remote
            ;;
    esac
}

# Start Ollama if not running
start_ollama_if_needed() {
    if check_ollama; then
        return 0
    fi
    
    # If using remote Ollama (like Windows from WSL), can't auto-start
    if is_remote_ollama; then
        print_color "$RED" "Error: Remote Ollama at ${OLLAMA_HOST} is not running."
        print_color "$YELLOW" "Make sure Ollama is running on the remote host."
        exit 1
    fi
    
    print_color "$YELLOW" "Starting Ollama..."
    
    # Start Ollama in background
    nohup ollama serve > /dev/null 2>&1 &
    
    # Wait for it to start (max 10 seconds)
    local attempts=0
    while [ $attempts -lt 20 ]; do
        sleep 0.5
        if check_ollama; then
            print_color "$GREEN" "[OK] Ollama started"
            return 0
        fi
        attempts=$((attempts + 1))
    done
    
    print_color "$RED" "Error: Could not start Ollama"
    print_color "$YELLOW" "Try running 'ollama serve' manually"
    exit 1
}

# Generate command from natural language
generate_command() {
    local prompt="$1"
    
    local system_prompt="You are a Linux command line expert. Convert the user's natural language request into a single, safe shell command. Rules: Output ONLY the command, nothing else. No explanations, no markdown, no code blocks. Use common, safe commands. Prefer non-destructive operations. If the request is unclear or potentially dangerous, output: echo Unable to generate safe command. Never output commands that could cause data loss without explicit user intent."

    # Escape special characters for JSON
    local escaped_prompt=$(echo "$prompt" | sed 's/\\/\\\\/g; s/"/\\"/g')
    
    local response=$(curl -s "${OLLAMA_HOST}/api/generate" \
        -H "Content-Type: application/json" \
        -d "{
            \"model\": \"${MODEL}\",
            \"prompt\": \"${escaped_prompt}\",
            \"system\": \"${system_prompt}\",
            \"stream\": false
        }" | jq -r '.response' 2>/dev/null)
    
    # Clean up the response - remove markdown code blocks, backticks, and whitespace
    response=$(echo "$response" | tr -d '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
    response=$(echo "$response" | sed 's/^```[a-z]*//;s/```$//;s/^`//;s/`$//')
    response=$(echo "$response" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
    
    echo "$response"
}

# Main function
main() {
    # Check for help flag
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
        echo "[llama] shlama-linux - Your terminal llama"
        echo ""
        echo "Usage: shlama \"<natural language request>\""
        echo ""
        echo "Commands:"
        echo "  shlama --model          Change the AI model"
        echo "  shlama --model show     Show current model"
        echo "  shlama --version        Show version"
        echo "  shlama --help           Show this help"
        echo ""
        echo "Examples:"
        echo "  shlama \"list all files including hidden\""
        echo "  shlama \"find all python files in current directory\""
        echo "  shlama \"show disk usage\""
        echo "  shlama \"install neofetch\""
        echo ""
        echo "Environment variables:"
        echo "  SHLAMA_MODEL    - Ollama model to use (overrides saved model)"
        echo "  OLLAMA_HOST     - Ollama API host (default: http://localhost:11434)"
        echo ""
        exit 0
    fi

    # Check for version flag
    if [[ "$1" == "-v" || "$1" == "--version" ]]; then
        echo "shlama-linux v1.1.0"
        exit 0
    fi

    # Check for model command
    if [[ "$1" == "--model" || "$1" == "-m" ]]; then
        if [[ "$2" == "show" || -z "$2" && "$1" == "--model" && -z "$2" ]]; then
            # Show current model
            if [[ "$2" == "show" ]]; then
                echo "Current model: $MODEL"
                if [[ -f "$CONFIG_FILE" ]]; then
                    echo "Saved in: $CONFIG_FILE"
                fi
                exit 0
            fi
        fi
        
        # Model selection menu
        echo ""
        print_color "$CYAN" "[robot] Select an AI model:"
        echo ""
        echo "  1) llama3.2      - Fast & light (~2GB) [Recommended]"
        echo "  2) llama3.2:1b   - Fastest, minimal (~1.3GB)"
        echo "  3) llama3        - Balanced (~4.7GB)"
        echo "  4) mistral       - Good quality (~4.1GB)"
        echo "  5) Custom        - Enter model name manually"
        echo ""
        echo "Current model: $MODEL"
        echo ""
        read -p "Select [1-5]: " choice
        
        case "$choice" in
            1) new_model="llama3.2" ;;
            2) new_model="llama3.2:1b" ;;
            3) new_model="llama3" ;;
            4) new_model="mistral" ;;
            5) 
                read -p "Enter model name: " new_model
                ;;
            *) 
                print_color "$RED" "Invalid choice"
                exit 1
                ;;
        esac
        
        # Save to config
        mkdir -p "$(dirname "$CONFIG_FILE")"
        echo "$new_model" > "$CONFIG_FILE"
        
        print_color "$GREEN" "[OK] Model changed to: $new_model"
        
        # Check if model is downloaded
        if ! ollama list 2>/dev/null | grep -q "$new_model"; then
            echo ""
            read -p "Model not downloaded. Download now? (y/N): " -n 1 -r
            echo ""
            if [[ $REPLY =~ ^[Yy]$ ]]; then
                print_color "$BLUE" "Downloading $new_model..."
                ollama pull "$new_model"
                print_color "$GREEN" "[OK] Model downloaded"
            fi
        fi
        exit 0
    fi

    # Check if a prompt was provided
    if [[ -z "$1" ]]; then
        print_color "$RED" "Error: Please provide a natural language request."
        echo "Usage: shlama \"<request>\""
        echo "Example: shlama \"list all files\""
        exit 1
    fi

    # Check dependencies
    if ! command -v curl &> /dev/null; then
        print_color "$RED" "Error: curl is required but not installed."
        exit 1
    fi

    if ! command -v jq &> /dev/null; then
        print_color "$RED" "Error: jq is required but not installed."
        print_color "$YELLOW" "Install it with: sudo apt install jq"
        exit 1
    fi

    # Start Ollama if not running
    start_ollama_if_needed

    # Generate the command
    print_color "$CYAN" "[llama] Thinking..."
    local suggested_command=$(generate_command "$1")

    if [[ -z "$suggested_command" || "$suggested_command" == "null" ]]; then
        print_color "$RED" "Error: Failed to generate command."
        exit 1
    fi

    # Display the suggested command
    echo ""
    print_color "$YELLOW" "Suggested command:"
    print_color "$GREEN" "$suggested_command"
    echo ""

    # Ask for confirmation
    read -p "Run command? (y/N): " -n 1 -r
    echo ""

    if [[ $REPLY =~ ^[Yy]$ ]]; then
        echo ""
        print_color "$BLUE" "Executing..."
        echo "-------------------------------------"
        eval "$suggested_command"
        echo "-------------------------------------"
        print_color "$GREEN" "[OK] Done"
    else
        print_color "$YELLOW" "Command not executed."
    fi
}

main "$@"
